2015-11-19
==========

- av_compare_ts(): does the same (hopefully) as my Rational class comparison operators


2015-11-18
==========

After spending two days debugging looking for the reason why video playback was stuttering, I finally tried out a new version of the FFmpeg libraries and hey, problem gone.

While debugging though, I found out that contrary to my belief, my AVFrames are *not* reference counted. According to the documentation, this means that every cloning operation will actually copy the data. If that is true, it's not good at all. What I'd like to know is whether there is a way to force frames to be ref counted. Also, it may be a good idea to check the behavior by stepping into the code - this means I'll have to rebuild the Shift Media Project.

If that somehow does not work, I'd have to re-implement my wrapper class hierarchy so that it does the reference counting itself.

A few observations:

- Playback is still not perfectly smooth when the console window is "below" the display window. This likely has to do with the way Windows synchronizes the presentation of changes across multiple windows.
  [Solved (for now): when the console window is brought to the foreground, or moved to the other screen, video display becomes smooth.]
  
- Colors are less saturated than with VLC. This is probably due to my color space conversion formulae. I should try some of the alternatives, and also have a look at that article I saw but didn't read in a recent issue of C't magazine.

- Video upscaling is not smooth. I'll have to see whether that requires work in the shader or can be done with a simple texture setting. [Done - all that was needed was to set the min and mag filters to GL_LINEAR]

2015-11-13
==========

Other than spending an awful lot of time on a really silly bug yesterday, I made another observation yesterday: even though the current implementation of the Demuxer does not yet obtain per-frame timestamps (they are all zero), the video does not play at "full steam ahead". In fact, it seems to play at exactly the right speed. I need to investigate.

...

Well searching around on SO didn't yield an answer to that, but it revealed a new..

TODO: flush the decoder by feeding it empty packets: http://stackoverflow.com/a/25533696/754534

-------

After drastically incrementing the size of the video frame queue in the Player class, it is clear now that the playing speed matching the video speed must have been pure coincidence (though it is a bit hard to believe): with increased queue size, video playback is now at roughly twice the speed, except in spots where the video decoder has to work harder than usual, such as leaving a tunnel.

Also, there still are times when playback just freezes for a second or two; it seems that the filesystem or the disk is the culprit here. This will probably be best addressed by implementing a queue on packets (under the proviso that decoding takes constant time, queueing packets should use up less space than queuing the decoded frames, as the packets carry data that is still compressed).

2015-11-11
==========

Getting multithreading to work correctly gave me a lot of trouble. It may be a good idea to create a reusable WorkerThread class that handles the communication part, i.e. at least command/acknowledge and thread-safe state queries, plus possibly notifications on state changes.

----------

YUVPainter faces a problem stemming from the fact that FFMpeg defines video format identifiers as an enum that contains conditional parts. This makes it impossible to rely on a specific format being identified by a specific number.

What this means that is that the YUVPainter class MUST be translated for the specific version of FFMPEG being employed, and that it if it is ever distributed in binary form, it should be bundled with the binaries of FFmpeg themselves. It would be more prudent, however, to add the YUVPainter module to a project in source form, so that it will automatically - bar an erroneous project setup - use the right values.

---

Addendum to the above: an alternative would be to find another way to describe the format to the YUV painter, one that does not involve an identifier.


2015-11-10
==========

An important lesson learned tonight: properly setting a condition variable always requires acquiring a lock first.

While waiting on a condition variable implicitly requires a lock, it is possible to forget acquiring a lock when setting a variable and notifying the other thread(s). The holy trinity however runs like this:

lock_guard<mutex> lk(the_mutex);
variable = VALUE;
condition.notify_one(); // or notify_all();

I think I saw somewhere that the lock may be unlocked prior to calling notify_xxx(), but on the other hand, all examples I'm seeing online always use the above form where the lock is still locked at notification time.
I do think however that the crucial item is using the mutex to protect modifications to the variable. The "other thread" needs to acquire the mutex to look at the variable - waiting however is optional, so the typical form used on the "other thread" is:

unique_lock<mutex> lk(the_mutex);
while (!variable == VALUE)
  condition.wait(lk);
  

2015-11-06
==========

The next step is now to actually display frames. Brainstorming.

- Start out with display only luma
- Use a shader from the very start
- There may be multiple shaders [depends on whether or not FFmpeg video decoders have normalized output]
- Those shaders belong into a GPC-AV module [might be extracted into their own module(s) in the future]
  - gpc/_av/opengl/shaders.hpp:
    - vanilla_vertex_shader
    - yuv_fragment_shader
- The "vanilla" vertex shader does nothing else than the usual transforms, both for vertices and texture coordinates
- The fragment shader uses 3 texture samplers
- More information is needed on the exact format delivered by video decoders
- Is it possible to compile a shader program with only the fragment shader ?
  -> NO (Stack Overflow answer)  
- In what form should the OpenGL YUV displaying functionality be provided?
  -> A class with:
    - appropriate hooks for lifecycle management (OpenGL contexts)
    - 3 queries for each of the 3 textures (Y,U,V)

2015-11-05
==========

I need to find a good way to work with time. Brainstorming.

- Timestamps on frames are rational numbers and often cannot be represented with complete precision by any type, integer or floating point.

- It would be great to be able to work with timestamps without knowing or caring how they're implemented.

- In order not to accumulate inaccuracies, operations involving divisions should always handle the rest explicitly.

- It would be nice if there was a way to make the conversions between different clock types automatic.

- For the time being though, working with doubles and seconds should be good enough.

----------------

- TODO: look at av_frame_get_best_effort_timestamp(frame)


2015-11-04
==========

TODO: it just occurred to me that executing the decoders from inside the mutex reader loop might waste the opportunity to have that work done in a separate thread. FFmpeg can't do that internally as the call is synchronous.

- TODO: find out if avformat_open_input() waits for necessary metadata before returning

----------

The handling of time
--------------------

How to handle time when playing a multimedia stream ? Brainstorming.

- Player presentation time starts elapsing when get_newest_video_frame() is called for the first time [or, in the future, any get_frame()] (1)

- If synchronization demands that playback be delayed, that delay must NOT be included in the playback time (quite natural, but will likely require special handling) [FUTURE FEATURE]

- Presentation timestamps of decoded frames will be used to determine when to replace the frame currently being displayed with the following one

- The player should provide a convenience function to determine whether the front frame in the buffer is due for presentation
  -> OR refuse to yield it until its presentation time has come ?
  
- (1) assumes that the time elapsing between dequeuing via get_newest_video_frame() and actual presentation is constant. This is not necessarily the case if, for example, get_newest_video_frame() is called at the end (or in the middle, for that matter) of the body of a rendering loop.
  This leads to a danger of the Player refusing to yield a new frame if the consumer attempts to get it only slightly earlier in its display cycle than when it obtained the first frame. This would be especially bad if the media stream actually has the same frame rate as the display, as it would lead to totally unnecessary hiccups.
  -> This could be solved by allowing the start of presentation time to "slip" forward a little - but by how much?
  
- Should the Player class try to determine the display framerate ?

- Maybe it would be good to have an abstraction of what MS calls the "swap chain"

- A special case that should also be provided for are free-sync monitors:
  a) output frame rate could be locked to the media stream frame rate
  b) rendering loop could provide an estimate of the time it will take for the frame to be presented; the player could then decide whether to stay on the current frame or move forward to the next one (if available)

------------

- Found when browsing: SDL2's vsync does not work when a window is minimized.

Ok, now to continue with the brainstorming for the handling of time.

- How to determine when to move on to the next frame ?
  -> Simple answer: look at the frame's timestamp: if it's older than or equal to the time elapsed since playing started, discard the presently displaying frame and get the new one. This can be repeated if there is a chance that the time elapsed since the last presentation is longer than the frame period.
  - The problem with the above is when v-sync is being used and the media framerate is close to or equal to the display framerate. It is then possible that frames are repeated and their predecessors repeated, because the time at which 

-------

Drew a diagram and came to the conclusion that vertical sync is probably the best way to get better control over sync problems.

BUT it is probably not needed anyway:
- When the rendering loop framerate is higher than the media framerate, with or without v-sync, what will happen is that somes frames will be repeated 1 time more often than others. This will not look too good, but there's not much to be done about it.
- What would happen when both framerates are close to each other is more complicated; this would likely result in quite noticeable stutter.

- For now, all that is really needed is a call that informs the player when the first frame has been presented (can later be extended to include vertical refresh rate, length of presentation chain, etc.). This will serve as a good basis to synchronize the sub-streams.

2015-11-03
==========

I have for now removed queues from the Decoder/VideoDecoder class (can't remember which it was). There is a design question here that needs answering before going further: how to control the Demuxer so that it doesn't read and decode packets faster than the consumer can use them? 

- Simply halting the demuxer thread would allow one consumer to prevent all others from being fed
  -> this might actually be desirable behaviour
  -> but it might also prevent the demuxer thread from receiving UDP packets [not sure whether FFmpeg has its own thread to receive and buffer incoming network packets]

- A query might be defined on Frame Sink (Consumer) objects that informs the upstream (Decoder) whether or not the Consumer is ready to receive more frames
  -> would imply that Frame Sinks can no longer be expressed via a single callback - is this unnecessary complexity?
  -> a similar query is needed on Packet Sinks (Decoders) to inform the Demuxer
  --> define a base class MediaSink, with a query method "ready()" ?
  - The Demuxer would then suspend the delivering of packets to Decoders if any of the Decoders doesn't report "ready"
  - The Demuxer will then go in a cycle of short sleep periods followed by retrials
    -> The problem with that is that the shortest sleep period supported by the OS might be much too long
  - OR the Demuxer could use a condition variable that holds the count of blocked Frame Sinks (Decoders), and wait on
    the condition variable to go back to 0 before resuming
    
Taking a step back. The above discussion assumes that Frame Sinks are allowed to be independent from each other, in the sense that there is no controlling instance to which the coordinating of production/consumption can be delegated, and that therefore the Demuxer must handle this itself.

This may not be a realistic or helpful assumption. In fact, letting such data chains churn away without a controlling instance, while an alluring thought, could lead to unnecessary restrictions, since safe operation would then likely require the enacting of strict rules.

The best way forward may be to define hold() and resume() methods on the Demuxer class. This methods must not however be confused with the classic pause() / play() (pseudo)-pair, which will likely not be implemented on the Demuxer, but on a future Player class. That Player class might well use hold() and resume() internally though, but it may need to complement them with further calls.

I think the best way forward right now is to implement a Player class, which will be immediately used in the test scene of the "Clock" screen sharing test program. Some or even most of that functionality may or may not be later extracted into a more generic base class, which could then also serve as the basis for a yet-to-be Trranscoder class.


2015-11-02
==========

It just occurred to me that there is no sense in passing AVFrame ownership around from the Decoder to the Consumer and back because I've now implemented copy functionality for the Frame wrapper class which creates clones of a Frame every time one is copied.

This means that frame objects can be put back into the "free" queue as soon as they have been presented to all the consumers.

---------

Upgrading the Pimpl idiom was, in the end, rather straightforward, though it does make things a little more complicated:

- The Impl structure of the base class (DecoderBase) has to be defined in a separate header file so that classes inheriting from it (the envelope) can define Implementations that inherit from the base class' Impl.
- Derived classes must provide a way to static_cast the Impl pointer to the type of their own, derived Impl. In this case, I renamed  the pointer - in the base class - to _p and defined an accessor method p() in the derived Envelope class (VideoDecoder) that does that casting.
  - This introduces a not-so-nice inconsistency between classes that have inheriting Impl's, needing to write p()->..., and those that do not, which can simply use p->...
    -> This could be fixed by simply defining that all Envelopes shall use p()

2015-11-01
==========

How to make it so that both canned (file) and live (streamed) video play correctly "out of the box", AND still support random frame access ?

How about introducing a Presenter object that takes care of these things ? Brainstorming:

- Presenter handles the synchronizing of multiple streams (i.e. buffering/discarding until timestamps match)
  - Only when starting consumption: discard frames or buffer slices that are older than the oldest available from all other streams (i.e. "earliest common timestamp")
- The Presenter also controls the Demuxer when reading from storage, telling it to pause when enough "advance" frames have been stored?
  -> Presenter assumes temporary ownership of Frames obtained from the Demuxer, only giving them back when it is done with them
    -> This may lead to the Demuxer "starving"
       -> Should the Demuxer be allowed to allocate more Frames?
       -> Or should this be considered an error, terminating the reader thread and throwing an exception in the consumer thread ?
  -> NO: the Demuxer's reader thread is an implementation detail; the interface only exposes pausing and seeking
     -> when reading from storage, it is the Presenter's responsibility to give back frames as soon as it is done with them
        -> the Demuxer starving should be considered a usage error (and thus, an internal error of the user code)
- Speaking about seeking:
  - If the target frame is not a key frame, Demuxer must seek back (does FFMPEG do that automatically?)
    -> will lead to delay, and re-synching of streams
       -> re-synching: may mean that seeking must go as far back as needed to fully compensate for streams that may be out of sync - does FFMPEG handle that too?
       -> should there be queriable states, such as "playing", "seeking", "sychronizing", etc. ? Does FFMPEG already have them?
  - It may make sense to continue playing buffered frames while the seeking is underway
      
Now to plan for the specific Presenter that is needed here: playing at nominal speed, no seeking (yet)

- Obtaining the frames as YUV, using a shader to display
- Keeping frames until they are no longer needed
- Letting the Demuxer wait until it is given back at least one Frame
- Remembers the real-world timestamp of the first frame (when starting to play or after a seek)
- Tries to determine the actual presentation time by using the Windows API ?
  -> There doesn't seem to be a call that does exactly that
  -> DwmGetCompositionTimingInfo() might help when the DWM is enabled
  -> SDL_DisplayMode has the refresh rate when SDL is used
  -> Couldn't find off-hand a way to obtain that info from Windows
  -> WON'T DO: too difficult for the time being
- The presenter will simply determine the current time [accuracy is 10ms, maybe this should be improved upon via timeBeginPeriod() / timeEndPeriod(): https://msdn.microsoft.com/en-us/library/dd757624(v=vs.85).aspx]
  - Upon presenting the first frame, the presenter shall calculate the difference to the presentation timestamp of that frame
  - There are now two possible modi operandi:
    1) The Presenter can be invoked periodically. This mode would work as follows: when present() is called,
       - the Presenter will calculate the theoretical timestamp of the frame that should be displayed *right now* (= the "target timestamp")
       - it will then go through available frames, from oldest to newest
         - if the presentation timestamp of the frame is older than the target timestamp, the frame is returned to the Demuxer UNLESS no newer frame is available
           - If playback is still in the beginning phase, this is very likely to happen. It means that the framerate of the media stream is lower than the framerate of the rendering loop.
           - It would be possible to simply ignore this and do nothing more than re-render the newest available frame. However, this would make it impossible to distinguish between the expected behaviour at start of playback and abnormal behaviour later on when playback is past the starting phase.
           - The correct way to handle this is to take the official stream framerate into account, and to register an anomaly only if the time elapsed since the last presentation exceeds one frame period (inverse of the framerate).
           - Another thing to do, when that keeps happening, is to introduce an artificial delay in the playback, giving the Demuxer some slack to compensate for difficulties
           
----------

Who drives who?

At present, I'm looking at: demuxer --(owned)--> decoder --(callback)--> presenter

The callback approach is pretty flexible, I just see it as a more advanced version of polymorphic invocation in a publish/subscribe pattern.


2015-10-29
==========

About queues, frames etc.
-------------------------

- Since individual streams within a stream bundle do not necessarily have the same packet rate, each stream must maintain its own queue.

- Therefore, queue sizes should be chosen carefully so as to avoid one queue being full while the other is still being filled.

- Right now, there is no time to properly handle synchronization, which is a complex subject.

- However, it ought to be possible to have the software compute queue sizes automatically from available metadata (frame rate, sample rate and size).

- For now, queues will be allowed to grow indefinitely (with an automatic abort when a generous limit is exceeded)